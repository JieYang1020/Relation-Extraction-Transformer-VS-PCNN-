# coding: utf-8
import torch
import torch.nn as nn
import numpy as np
import math
import torch.nn.functional as F
from .BasicModule import BasicModule

'''
分类问题不用decoder
'''
####################scaled dot_product attention####################
class ScaledDotProductAttention(nn.Module):
    """Scaled dot-product attention mechanism"""
    
    def __init__(self, attention_dropout=0.0):
        super(ScaledDotProductAttention, self).__init__()
        self.dropout = nn.Dropout(attention_dropout)
        self.softmax = nn.Softmax(dim = 2)
    
    def forward(self, q, k, v, scale = None, attn_mask = None):
        '''前向传播。scale:缩放因子（1/(根号K)），一个浮点标量。返回：上下文张量和
        attention张量'''
        attention = torch.matmul(q, k.transpose(1, 2))
        #attention.shape[batch_size*num_heads, max_len, max_len]
        #attn_mask.shape[batch_size*num_heads, max_len, max_len]
        if scale:
            attention = attention * scale
        # if attn_mask:
            #给需要mask的地方设置负无穷
        attention = attention.masked_fill_(attn_mask, -np.inf)
        #计算softmax
        attention = self.softmax(attention)
        #添加dropout
        attention = self.dropout(attention)
        #和V做点积
        context = torch.bmm(attention, v)
        return context, attention

####################multi-head attention####################
#将Q,K,V拆分为8份(heads=8)，每份分别进行scaled dot-product attention
class MultiHeadAttention(nn.Module):
    
    def __init__(self, model_dim=64, num_heads=2, dropout=0.0):
        super(MultiHeadAttention, self).__init__()
        #dim_per_head:q, k, v向量的长度.Wq, Wk, Wv 的矩阵尺寸为model_dim/dim_per_head
        self.dim_per_head = model_dim // num_heads
        self.num_heads = num_heads
        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)
        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)
        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)
        
        self.dot_product_attention = ScaledDotProductAttention(dropout)
        self.linear_final = nn.Linear(model_dim, model_dim)
        self.dropout = nn.Dropout(dropout)
        # multi_head attention之后需要做layer norm
        self.layer_norm = nn.LayerNorm(model_dim)
    
    def forward(self, key, value, query, attn_mask = None):
        #残差连接，增加任意常数，求导为1，避免反向传播时的梯度消失
        residual = query
        dim_per_head = self.dim_per_head
        num_heads = self.num_heads
        batch_size = key.size(0)
        #linear projection
        key = self.linear_k(key)
        value = self.linear_v(value)
        query = self.linear_q(query)
        #split by heads
        key = key.view(batch_size*num_heads, -1, dim_per_head)
        value = value.view(batch_size*num_heads, -1, dim_per_head)
        query = query.view(batch_size*num_heads, -1, dim_per_head)
        # if attn_mask:
        #repeat是为了能在dot_product_attention中做mask_fill
        attn_mask = attn_mask.repeat(num_heads, 1, 1)
        scale = (key.size(-1) // num_heads) ** -0.5
        #调用之前的scaled dot product attention
        context, attention = self.dot_product_attention(query, key, value,
                                                        scale, attn_mask)
        #将8个头的结果连接
        context = context.view(batch_size, -1, dim_per_head * num_heads)
        #final linear projection
        output = self.linear_final(context)
        #dropout
        output = self.dropout(output)
        #add residual and norm layer
        residual = residual.squeeze(1)
        output = self.layer_norm(residual + output)
        return output, attention

####################残差连接####################
def residual(sublayer_fn, x):
    return sublayer_fn(x)+x
####################mask####################
#encoding的时候需要先padding
def padding_mask(seq_k, seq_q):
    len_q = seq_q.size(1)
    pad_mask = seq_k.eq(0)
    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)
    return pad_mask

####################position-wise feed-forwad network####################
import torch
import torch.nn as nn
class PositionalWiseFeedForward(nn.Module):
    def __init__(self, model_dim=64, ffn_dim=2048, dropout=0.0):
        super(PositionalWiseFeedForward, self).__init__()
        self.w1 = nn.Conv1d(model_dim, ffn_dim, 1)
        self.w2 = nn.Conv1d(ffn_dim, model_dim, 1)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(model_dim)
    def forward(self, x):
        # torch.Size([batch_size, max_len, model_dim])
        output = x.transpose(1, 2)
        output = self.w2(F.relu(self.w1(output)))
        output = self.dropout(output.transpose(1,2))
        #add residual and norm layer
        output = self.layer_norm(x + output)
        return output

####################PositionalEncoding层####################
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_seq_len):
        """初始化。

        Args:
            d_model: 一个标量。模型的维度，论文默认是512
            max_seq_len: 一个标量。文本序列的最大长度
        """
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=0.0)
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0., max_seq_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0., d_model, 2) *
                             (math.log(10000.0)/d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        standard_value = self.pe[:, :x.size(1)].expand(x.size(0), 82, 64)
        x = x + torch.autograd.Variable(standard_value,
                                        requires_grad =False)
        return self.dropout(x)

####################Encoder层####################
class EncoderLayer(nn.Module):
    def __init__(self, model_dim=64, num_heads=2, ffn_dim=2018,
                 dropout=0.0):
        super(EncoderLayer, self).__init__()
        self.attention = MultiHeadAttention(model_dim, num_heads, dropout)
        self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout)
    def forward(self, embedding_output,self_attention_mask, attn_mask = None):
        #self attention
        #inputs.shape=[2,82,512], padding_mask.shape=[2,82,82]
        #padding应该是只对词本身padding
        context, attention = self.attention(embedding_output, embedding_output,
                                            embedding_output, self_attention_mask)
        #feed forward network
        output = self.feed_forward(context)
        return output, attention
class Encoder(nn.Module):
    '''多层EncoderLayer组成Encoder'''
    def __init__(self, vocab_size=160695 + 2, max_seq_len=80 + 2,
                 num_layers=6, model_dim=64,
                 num_heads=2, ffn_dim=2048, dropout=0.2):
        super(Encoder, self).__init__()
        self.encoder_layers = nn.ModuleList([EncoderLayer(model_dim, num_heads,
                                                          ffn_dim,dropout) for 
                                            _ in range(num_layers)])
        self.seq_embedding = nn.Embedding(vocab_size + 1, model_dim,
                                          padding_idx = 0)
        self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)
    def forward(self, inputs, insPF1, insPF2):
        output_context = self.seq_embedding(inputs)
        # 应先做了positional embedding(为了维度扩展), 再做positional encoding, 再将PF1,PF2合并
        output_pos1 = self.pos_embedding(self.seq_embedding(insPF1))
        output_pos2 = self.pos_embedding(self.seq_embedding(insPF2))
        #三个结果衔接,衔接后size为[2, 82, 512]
        embedding_output = output_context + (output_pos1 + output_pos2)
        #q,k,v的size应为[batch_size, max_len, h, model_dim]
        embedding_output = embedding_output.unsqueeze(1)
        self_attention_mask = padding_mask(inputs, inputs)
        attentions = []
        for encoder in self.encoder_layers:
            output, attention = encoder(embedding_output, self_attention_mask)
            attentions.append(attention)
        return output, attentions

###################组合Encoder和Decoder层####################
class Transformer_self(BasicModule):
    def __init__(self, opt):
        super(Transformer_self, self).__init__()
        self.model_name = 'Transformer_self'
        self.opt = opt
        self.src_vocab_size = self.opt.vocab_size
        self.src_max_len = self.opt.max_len
        self.num_layers = 1; self.model_dim = 64; self.num_heads = 2
        self.ffn_dim = 2048; self.dropout = 0.2
        self.encoder = Encoder(self.src_vocab_size, self.src_max_len, self.num_layers,
            self.model_dim, self.num_heads, self.ffn_dim, self.dropout)
        self.linear = nn.Linear(self.model_dim, self.opt.rel_num, bias = False)
        self.softmax = nn.Softmax(dim = 1)
    def forward(self, inputs):
        insEnt, _, insX, insPFs, insPool, insMasks = inputs
        #insX是input_vector,size = [3,82]。 insPFs是位置信息, size = [3,2,82]
        insPF1, insPF2 = [i.squeeze(1) for i in torch.split(insPFs, 1, 1)]
        #之后分别进入positionalEncoding层
        output, enc_self_attn = self.encoder(insX, insPF1, insPF2)

        #全连接前降维(使用max将max_len去掉)
        output = torch.max(output, dim = 1).values
        # output = output.view(output.size(0), -1)
        output = self.linear(output)
        output = self.softmax(output)
        return output

